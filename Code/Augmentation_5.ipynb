{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1dc437",
   "metadata": {},
   "source": [
    "# Augmentation_5\n",
    "When synthesizing new data, 70% of new data are synthesized from 3 data that are far from centorid. Rest 30% of new data are synthesized from data that are close to centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7f53e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edd5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddfc766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "data = pd.read_csv(\"./dataset.csv\")\n",
    "pd.set_option('display.max_row',None)\n",
    "pd.set_option('display.max_column',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1571b77",
   "metadata": {},
   "source": [
    "## Separate Petting/Non-Petting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e24343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the datasets Petting and the datasets that are not Petting\n",
    "petting_d= data[data['Petting'] == 1]\n",
    "non_petting_d= data[data['Petting'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41186ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets preprocessing\n",
    "def data_preprocessing(data):\n",
    "    data.reset_index(inplace=True)\n",
    "    data=data.drop(['index'],axis=1)\n",
    "    data= data.drop(['Eating / Cooking', 'Moved It', 'Petting','TV / Radio','Talking'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84ff18",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d2511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTE:\n",
    "    def __init__(self,data,k,pad,synth_number,skew):\n",
    "        self.data=data\n",
    "        self.k=k\n",
    "        self.pad=pad\n",
    "        self.synth_n=synth_number\n",
    "        self.skew = skew\n",
    "        \n",
    "    # Make the dataframe into list and put the index\n",
    "    def making_list(self,data):\n",
    "        data_list=data.values.tolist()\n",
    "        a=0\n",
    "        for x in data_list:\n",
    "            x.insert(0,a)                                       #put the index of the data in the front\n",
    "            a+=1\n",
    "        return data_list\n",
    "\n",
    "    # Calculate eucledian distance\n",
    "    def eucledian(self,p1,p2):\n",
    "        dist = np.sqrt(np.sum((p1-p2)**2))\n",
    "        return dist\n",
    "    \n",
    "    # SMOTE algorithm\n",
    "    def smote_al (self):\n",
    "        data=self.making_list(self.data)\n",
    "        feat_cnt = len(data[0])-1\n",
    "        feat_start = 1                                          #start position of features in dataset, usually first column are IDs\n",
    "        data_avg = [0 for i in range(feat_start+feat_cnt)] \n",
    "        data_sd = [0 for i in range(feat_start+feat_cnt)]\n",
    "        data_med = [0 for i in range(feat_start+feat_cnt)]      #med = median\n",
    "        data_skew = [0 for i in range(feat_start+feat_cnt)]\n",
    "        k = self.k                                               #number of nearest neighbors (e.g. 3)\n",
    "        pad = self.pad                                           #Padding - percentage of std dev to add to each feature (2 = 95th percentile)\n",
    "        skew_x = self.skew                                       #Skew factor ... >1 increases skew, <1 decreases skew, =1 no effect\n",
    "        synth_n = self.synth_n                                   #number of synthetic samples to make\n",
    "        synth = [[] for i in range(synth_n)]                    #placeholder array for synthetic dataset\n",
    "        nn= [[] for i in range(len(data))]\n",
    "        \n",
    "        #Find nearest neighbors\n",
    "        dist_list=[]                              \n",
    "        for i in data:                                      \n",
    "            idx = i[0]                                          #get current sample index\n",
    "            for m in range(len(data)):\n",
    "                dist=self.eucledian(np.array(data[idx][1:]),np.array(data[m][1:])) #get the eucledian distance between the current data and other data\n",
    "                a=(dist,idx,m)\n",
    "                dist_list.append(list(a))\n",
    "            dist_list.sort(key=lambda x:x[0])                  #sort the list by the distance\n",
    "            for j in range(k):\n",
    "                nn[idx].append(dist_list[j+1][2])               #append the data index\n",
    "            dist_list=[]\n",
    "        \n",
    "        # Calc std devs \n",
    "        '''calculate the std dev for feature j across all rows of data, put in data_sd array'''\n",
    "        for j in range(feat_start, feat_start+feat_cnt):\n",
    "            temp_arr=[]\n",
    "            for i in range(len(data)):\n",
    "                temp_arr.append(data[i][j])                     #if dataset is numpy array, this could be done more cleanly w/o pulling data out to temp array\n",
    "            data_avg[j] = np.mean(temp_arr)\n",
    "            data_sd[j] = np.std(temp_arr)\n",
    "            data_med[j] = np.median(temp_arr)\n",
    "        \n",
    "        #Calc skewness\n",
    "        '''calculate skewness of dataset for each feature j'''\n",
    "        for j in range(feat_start, feat_start+feat_cnt):\n",
    "            if data_avg[j]-data_med[j]==0 :  # when avg, med, sd = 1.0, 1.0, 0.0\n",
    "                data_skew[j] = 0\n",
    "            else:\n",
    "                data_skew[j] = (data_avg[j]-data_med[j])/ data_sd[j]\n",
    "        \n",
    "        # Find the centroid and pick 3 values that are far from centroid\n",
    "        bor_list=[]\n",
    "        bor=[]\n",
    "        data_index=[]\n",
    "        for i in data:                                      \n",
    "            idx = i[0]\n",
    "            data_index.append(idx)\n",
    "            dist=self.eucledian(np.array(data[idx][1:]),data_avg[1:])\n",
    "            b=(dist,idx)\n",
    "            bor_list.append(list(b))\n",
    "        bor_list.sort(key=lambda x:x[0],reverse=True)\n",
    "        for i in range (3):                                       #3 values that are far from centroid\n",
    "            bor.append(bor_list[i][1])\n",
    "        sub_bor = [x for x in data_index if x not in bor]\n",
    "        \n",
    "        # 70% of new data are synthesized from bor / 30% of new data are synthesized from sub_bor\n",
    "        index_choice=[]\n",
    "        for i in range(int(synth_n*0.7)):                         # 70% of new data\n",
    "            randomnum=rand.choice(bor)\n",
    "            index_choice.append(randomnum)\n",
    "        for j in range(synth_n-int(synth_n*0.7)):                 # 30% of new data\n",
    "            randomnum=rand.choice(sub_bor)\n",
    "            index_choice.append(randomnum)\n",
    "\n",
    "        # Create synthetic samples\n",
    "        for i in range(synth_n):    \n",
    "            idx = index_choice[i]                      #get random sample index\n",
    "            for j in range(feat_start, feat_start+feat_cnt):\n",
    "                diff = 0                                            #variable for holding difference between nearest neighbor\n",
    "                gap = rand.uniform(-1.0,1.0)                        #random decimal number between -1 and 1\n",
    "                for q in range(k):\n",
    "                    temp = nn[idx][q]\n",
    "                    diff += abs(data[temp][j]) - abs(data[idx][j])      #sum absolute difference across all nearest neighbors\n",
    "                diff = diff/k                                           #divide total by k to get avg diff\n",
    "                diff = diff + (data_sd[j]*pad)                          #add in the std_dev * padding  to difference to create perturbations                \n",
    "                gap2 = rand.uniform(0, 1.0)                             #random decimal number between -1 and 1\n",
    "                skew_shift = (data_skew[j] * skew_x) - data_skew[j]     #calc skew_shift based on skew_x value\n",
    "                skew_shift = data_avg[j] * skew_shift * gap2            #Apply skew_shift to average value, to get shift in unit_scale                \n",
    "                val = data[idx][j] + (diff*gap) + skew_shift\n",
    "                synth[i].append(val)\n",
    "\n",
    "        # Make into Dataframe\n",
    "        '''Append synth array onto the original dataset array'''\n",
    "        len_data=len(self.data)\n",
    "        for i in range(self.synth_n):\n",
    "            self.data.loc[len_data+i]=synth[i]\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a445b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging petting and non_petting datasets into one dataframe\n",
    "def into_one_dataframe(k, pad, synth_size, skew, t_1, t_0):\n",
    "    p=SMOTE(t_1, k, pad, synth_size+5, skew)\n",
    "    q=SMOTE(t_0, k, pad, synth_size, skew)\n",
    "    petting_data=p.smote_al()\n",
    "    non_petting_data=q.smote_al()\n",
    "    petting_data['Petting'] = 1\n",
    "    non_petting_data['Petting'] = 0\n",
    "    merged_data=pd.concat([petting_data,non_petting_data]) \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3d779",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebffe4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20857566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(data):\n",
    "    # Split the data into features (X) and target (y)\n",
    "    X = data.drop('Petting', axis=1)\n",
    "    y = data['Petting']\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    #Random Forest\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_pred_prob = rf.predict_proba(X_test)[:, 1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    return accuracy, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c1bd0",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4673cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(k, pad, synth_size, skew, t_1, t_0):\n",
    "    data_after_smote=into_one_dataframe(k, pad, synth_size, skew, t_1, t_0)\n",
    "    rf_acc, rf_auc = random_forest(data_after_smote)\n",
    "    return rf_acc, rf_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047589c",
   "metadata": {},
   "source": [
    "## Check ACC, AUC for each parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe61f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_auc(k, pad, synth_size, skew, t_1, t_0):\n",
    "    acc_Avg = 0\n",
    "    auc_Avg = 0\n",
    "    for i in range(5):\n",
    "        acc, auc = main(k, pad, synth_size, skew, t_1.copy(), t_0.copy())        \n",
    "        acc_Avg += acc\n",
    "        auc_Avg += auc\n",
    "    acc_Avg = acc_Avg/5\n",
    "    auc_Avg = auc_Avg/5\n",
    "    print('for k: ', k,' pad: ', pad,' synth_size: ', synth_size, 'for skew: ', skew)\n",
    "    print('average_accuaracy : ', round(acc_Avg, 5))\n",
    "    print('average_auc : ', round(auc_Avg, 5))\n",
    "    return acc_Avg, auc_Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea60055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=[]\n",
    "petting=data_preprocessing(petting_d)\n",
    "non_petting=data_preprocessing(non_petting_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37eb571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one by one \n",
    "# [k, pad, synth_size, skew]\n",
    "params_list = [[2, 1, 40, 0.01], [2, 1, 40, 0.5], [2, 1, 40, 1.0], [2, 1, 40, 1.5], [2, 1, 40, 2.0],\n",
    "              [2, 1, 80, 0.01], [2, 1, 80, 0.5], [2, 1, 80, 1.0], [2, 1, 80, 1.5], [2, 1, 80, 2.0],\n",
    "              [2, 2, 40, 0.01], [2, 2, 40, 0.5], [2, 2, 40, 1.0], [2, 2, 40, 1.5], [2, 2, 40, 2.0],\n",
    "              [2, 2, 80, 0.01], [2, 2, 80, 0.5], [2, 2, 80, 1.0], [2, 2, 80, 1.5], [2, 2, 80, 2.0],\n",
    "              [3, 1, 40, 0.01], [3, 1, 40, 0.5], [3, 1, 40, 1.0], [3, 1, 40, 1.5], [3, 1, 40, 2.0],\n",
    "              [3, 1, 80, 0.01], [3, 1, 80, 0.5], [3, 1, 80, 1.0], [3, 1, 80, 1.5], [3, 1, 80, 2.0],\n",
    "              [3, 2, 40, 0.01], [3, 2, 40, 0.5], [3, 2, 40, 1.0], [3, 2, 40, 1.5], [3, 2, 40, 2.0],\n",
    "              [3, 2, 80, 0.01], [3, 2, 80, 0.5], [3, 2, 80, 1.0], [3, 2, 80, 1.5], [3, 2, 80, 2.0],\n",
    "              [4, 1, 40, 0.01], [4, 1, 40, 0.5], [4, 1, 40, 1.0], [4, 1, 40, 1.5], [4, 1, 40, 2.0],\n",
    "              [4, 1, 80, 0.01], [4, 1, 80, 0.5], [4, 1, 80, 1.0], [4, 1, 80, 1.5], [4, 1, 80, 2.0],\n",
    "              [4, 2, 40, 0.01], [4, 2, 40, 0.5], [4, 2, 40, 1.0], [4, 2, 40, 1.5], [4, 2, 40, 2.0],\n",
    "              [4, 2, 80, 0.01], [4, 2, 80, 0.5], [4, 2, 80, 1.0], [4, 2, 80, 1.5], [4, 2, 80, 2.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc276c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for k:  2  pad:  1  synth_size:  40 for skew:  0.01\n",
      "average_accuaracy :  0.9\n",
      "average_auc :  0.93952\n",
      "for k:  2  pad:  1  synth_size:  40 for skew:  0.5\n",
      "average_accuaracy :  0.92\n",
      "average_auc :  0.9687\n",
      "for k:  2  pad:  1  synth_size:  40 for skew:  1.0\n",
      "average_accuaracy :  0.85\n",
      "average_auc :  0.91265\n",
      "for k:  2  pad:  1  synth_size:  40 for skew:  1.5\n",
      "average_accuaracy :  0.94\n",
      "average_auc :  0.95673\n",
      "for k:  2  pad:  1  synth_size:  40 for skew:  2.0\n",
      "average_accuaracy :  0.91\n",
      "average_auc :  0.9628\n",
      "for k:  2  pad:  1  synth_size:  80 for skew:  0.01\n",
      "average_accuaracy :  0.95\n",
      "average_auc :  0.96232\n",
      "for k:  2  pad:  1  synth_size:  80 for skew:  0.5\n",
      "average_accuaracy :  0.98333\n",
      "average_auc :  0.99595\n",
      "for k:  2  pad:  1  synth_size:  80 for skew:  1.0\n",
      "average_accuaracy :  0.91667\n",
      "average_auc :  0.9659\n",
      "for k:  2  pad:  1  synth_size:  80 for skew:  1.5\n",
      "average_accuaracy :  0.96667\n",
      "average_auc :  0.98049\n",
      "for k:  2  pad:  1  synth_size:  80 for skew:  2.0\n",
      "average_accuaracy :  0.95556\n",
      "average_auc :  0.97001\n",
      "for k:  2  pad:  2  synth_size:  40 for skew:  0.01\n",
      "average_accuaracy :  0.91\n",
      "average_auc :  0.94651\n",
      "for k:  2  pad:  2  synth_size:  40 for skew:  0.5\n",
      "average_accuaracy :  0.91\n",
      "average_auc :  0.93363\n",
      "for k:  2  pad:  2  synth_size:  40 for skew:  1.0\n",
      "average_accuaracy :  0.82\n",
      "average_auc :  0.9485\n",
      "for k:  2  pad:  2  synth_size:  40 for skew:  1.5\n",
      "average_accuaracy :  0.91\n",
      "average_auc :  0.94918\n",
      "for k:  2  pad:  2  synth_size:  40 for skew:  2.0\n",
      "average_accuaracy :  0.93\n",
      "average_auc :  0.97872\n",
      "for k:  2  pad:  2  synth_size:  80 for skew:  0.01\n",
      "average_accuaracy :  0.93889\n",
      "average_auc :  0.97759\n",
      "for k:  2  pad:  2  synth_size:  80 for skew:  0.5\n",
      "average_accuaracy :  0.94444\n",
      "average_auc :  0.98699\n",
      "for k:  2  pad:  2  synth_size:  80 for skew:  1.0\n",
      "average_accuaracy :  0.89444\n",
      "average_auc :  0.94379\n",
      "for k:  2  pad:  2  synth_size:  80 for skew:  1.5\n",
      "average_accuaracy :  0.95556\n",
      "average_auc :  0.98628\n",
      "for k:  2  pad:  2  synth_size:  80 for skew:  2.0\n",
      "average_accuaracy :  0.97222\n",
      "average_auc :  0.9952\n",
      "for k:  3  pad:  1  synth_size:  40 for skew:  0.01\n",
      "average_accuaracy :  0.95\n",
      "average_auc :  0.94836\n",
      "for k:  3  pad:  1  synth_size:  40 for skew:  0.5\n",
      "average_accuaracy :  0.91\n",
      "average_auc :  0.97445\n",
      "for k:  3  pad:  1  synth_size:  40 for skew:  1.0\n",
      "average_accuaracy :  0.87\n",
      "average_auc :  0.97104\n",
      "for k:  3  pad:  1  synth_size:  40 for skew:  1.5\n",
      "average_accuaracy :  0.89\n",
      "average_auc :  0.92263\n",
      "for k:  3  pad:  1  synth_size:  40 for skew:  2.0\n",
      "average_accuaracy :  0.88\n",
      "average_auc :  0.97778\n",
      "for k:  3  pad:  1  synth_size:  80 for skew:  0.01\n",
      "average_accuaracy :  0.95556\n",
      "average_auc :  0.99121\n",
      "for k:  3  pad:  1  synth_size:  80 for skew:  0.5\n",
      "average_accuaracy :  0.97222\n",
      "average_auc :  0.99625\n",
      "for k:  3  pad:  1  synth_size:  80 for skew:  1.0\n",
      "average_accuaracy :  0.92778\n",
      "average_auc :  0.98113\n",
      "for k:  3  pad:  1  synth_size:  80 for skew:  1.5\n",
      "average_accuaracy :  0.92778\n",
      "average_auc :  0.97305\n",
      "for k:  3  pad:  1  synth_size:  80 for skew:  2.0\n",
      "average_accuaracy :  0.94444\n",
      "average_auc :  0.99064\n",
      "for k:  3  pad:  2  synth_size:  40 for skew:  0.01\n",
      "average_accuaracy :  0.97\n",
      "average_auc :  0.98392\n",
      "for k:  3  pad:  2  synth_size:  40 for skew:  0.5\n",
      "average_accuaracy :  0.97\n",
      "average_auc :  0.98681\n",
      "for k:  3  pad:  2  synth_size:  40 for skew:  1.0\n",
      "average_accuaracy :  0.85\n",
      "average_auc :  0.90889\n",
      "for k:  3  pad:  2  synth_size:  40 for skew:  1.5\n",
      "average_accuaracy :  0.98\n",
      "average_auc :  1.0\n",
      "for k:  3  pad:  2  synth_size:  40 for skew:  2.0\n",
      "average_accuaracy :  0.94\n",
      "average_auc :  0.96668\n",
      "for k:  3  pad:  2  synth_size:  80 for skew:  0.01\n",
      "average_accuaracy :  0.96111\n",
      "average_auc :  0.97689\n",
      "for k:  3  pad:  2  synth_size:  80 for skew:  0.5\n",
      "average_accuaracy :  0.95\n",
      "average_auc :  0.96726\n",
      "for k:  3  pad:  2  synth_size:  80 for skew:  1.0\n",
      "average_accuaracy :  0.9\n",
      "average_auc :  0.95632\n",
      "for k:  3  pad:  2  synth_size:  80 for skew:  1.5\n",
      "average_accuaracy :  0.93333\n",
      "average_auc :  0.9712\n",
      "for k:  3  pad:  2  synth_size:  80 for skew:  2.0\n",
      "average_accuaracy :  0.94444\n",
      "average_auc :  0.98369\n",
      "for k:  4  pad:  1  synth_size:  40 for skew:  0.01\n",
      "average_accuaracy :  0.93\n",
      "average_auc :  0.98331\n",
      "for k:  4  pad:  1  synth_size:  40 for skew:  0.5\n",
      "average_accuaracy :  0.95\n",
      "average_auc :  1.0\n",
      "for k:  4  pad:  1  synth_size:  40 for skew:  1.0\n",
      "average_accuaracy :  0.8\n",
      "average_auc :  0.89797\n",
      "for k:  4  pad:  1  synth_size:  40 for skew:  1.5\n",
      "average_accuaracy :  0.95\n",
      "average_auc :  0.97674\n",
      "for k:  4  pad:  1  synth_size:  40 for skew:  2.0\n",
      "average_accuaracy :  0.95\n",
      "average_auc :  0.94299\n"
     ]
    }
   ],
   "source": [
    "for param in params_list:\n",
    "    temp_acc, temp_auc = check_acc_auc(param[0], param[1], param[2], param[3], petting, non_petting)\n",
    "    data_list.append([param[0], param[1], param[2], param[3], temp_acc, temp_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_auc_summary_df = pd.DataFrame(data=data_list, \n",
    "                                  columns = ['k', 'pad', 'synth_size', 'skewed', 'average_accuracy', 'average_auc'])\n",
    "acc_auc_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_auc_summary_df.to_csv('./augmentation_5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
