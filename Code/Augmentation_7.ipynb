{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1dc437",
   "metadata": {},
   "source": [
    "# Augmentation_7\n",
    "Custom Ensemble: Created a different synthesized dataset for each tree in the ensemble, using slightly different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7f53e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8edd5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cddfc766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "data = pd.read_csv(\"./dataset.csv\")\n",
    "pd.set_option('display.max_row',None)\n",
    "pd.set_option('display.max_column',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1571b77",
   "metadata": {},
   "source": [
    "## Separate Petting/Non-Petting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17e24343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the datasets Petting and the datasets that are not Petting\n",
    "petting_d= data[data['Petting'] == 1]\n",
    "non_petting_d= data[data['Petting'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41186ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets preprocessing\n",
    "def data_preprocessing(data):\n",
    "    data.reset_index(inplace=True)\n",
    "    data=data.drop(['index'],axis=1)\n",
    "    data= data.drop(['Eating / Cooking', 'Moved It', 'Petting','TV / Radio','Talking'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84ff18",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29d2511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTE:\n",
    "    def __init__(self,data,k,pad,synth_number,skew):\n",
    "        self.data=data\n",
    "        self.k=k\n",
    "        self.pad=pad\n",
    "        self.synth_n=synth_number\n",
    "        self.skew = skew\n",
    "        \n",
    "    # Make the dataframe into list and put the index\n",
    "    def making_list(self,data):\n",
    "        data_list=data.values.tolist()\n",
    "        a=0\n",
    "        for x in data_list:\n",
    "            x.insert(0,a)                                       #put the index of the data in the front\n",
    "            a+=1\n",
    "        return data_list\n",
    "\n",
    "    # Calculate eucledian distance\n",
    "    def eucledian(self,p1,p2):\n",
    "        dist = np.sqrt(np.sum((p1-p2)**2))\n",
    "        return dist\n",
    "    \n",
    "    # SMOTE algorithm\n",
    "    def smote_al (self):\n",
    "        data=self.making_list(self.data)\n",
    "        feat_cnt = len(data[0])-1\n",
    "        feat_start = 1                                          #start position of features in dataset, usually first column are IDs\n",
    "        data_avg = [0 for i in range(feat_start+feat_cnt)] \n",
    "        data_sd = [0 for i in range(feat_start+feat_cnt)]\n",
    "        data_med = [0 for i in range(feat_start+feat_cnt)]      #med = median\n",
    "        data_skew = [0 for i in range(feat_start+feat_cnt)]\n",
    "        k = self.k                                               #number of nearest neighbors (e.g. 3)\n",
    "        pad = self.pad                                           #Padding - percentage of std dev to add to each feature (2 = 95th percentile)\n",
    "        skew_x = self.skew                                       #Skew factor ... >1 increases skew, <1 decreases skew, =1 no effect\n",
    "        synth_n = self.synth_n                                   #number of synthetic samples to make\n",
    "        synth = [[] for i in range(synth_n)]                    #placeholder array for synthetic dataset\n",
    "        nn= [[] for i in range(len(data))]\n",
    "        \n",
    "        #Find nearest neighbors\n",
    "        dist_list=[]                              \n",
    "        for i in data:                                      \n",
    "            idx = i[0]                                          #get current sample index\n",
    "            for m in range(len(data)):\n",
    "                dist=self.eucledian(np.array(data[idx][1:]),np.array(data[m][1:])) #get the eucledian distance between the current data and other data\n",
    "                a=(dist,idx,m)\n",
    "                dist_list.append(list(a))\n",
    "            dist_list.sort(key=lambda x:x[0])                  #sort the list by the distance\n",
    "            for j in range(k):\n",
    "                nn[idx].append(dist_list[j+1][2])               #append the data index\n",
    "            dist_list=[]\n",
    "        \n",
    "        # Calc std devs \n",
    "        '''calculate the std dev for feature j across all rows of data, put in data_sd array'''\n",
    "        for j in range(feat_start, feat_start+feat_cnt):\n",
    "            temp_arr=[]\n",
    "            for i in range(len(data)):\n",
    "                temp_arr.append(data[i][j])                     #if dataset is numpy array, this could be done more cleanly w/o pulling data out to temp array\n",
    "            data_avg[j] = np.mean(temp_arr)\n",
    "            data_sd[j] = np.std(temp_arr)\n",
    "            data_med[j] = np.median(temp_arr)\n",
    "        \n",
    "        #Calc skewness\n",
    "        '''calculate skewness of dataset for each feature j'''\n",
    "        for j in range(feat_start, feat_start+feat_cnt):\n",
    "            if data_avg[j]-data_med[j]==0 :  # when avg, med, sd = 1.0, 1.0, 0.0\n",
    "                data_skew[j] = 0\n",
    "            else:\n",
    "                data_skew[j] = (data_avg[j]-data_med[j])/ data_sd[j]\n",
    "        \n",
    "        # Create synthetic samples\n",
    "        '''Randomly selecting samples, then use the difference between nearest neighbors and std dev to calcuate new feature values with perturbation'''\n",
    "        for i in range(synth_n): \n",
    "            idx = rand.randint(0, len(data)-1)                      #get random sample index            \n",
    "            for j in range(feat_start, feat_start+feat_cnt):\n",
    "                diff = 0                                            #variable for holding difference between nearest neighbor\n",
    "                gap = rand.uniform(-1.0,1.0)\n",
    "                for q in range(k):\n",
    "                    temp = nn[idx][q]\n",
    "                    diff += abs(data[temp][j]) - abs(data[idx][j])      #sum absolute difference across all nearest neighbors\n",
    "                diff = diff/k                                           #divide total by k to get avg diff\n",
    "                diff = diff + (data_sd[j]*pad)                          #add in the std_dev * padding  to difference to create perturbations                \n",
    "                gap2 = rand.uniform(0, 1.0)                             #random decimal number between -1 and 1\n",
    "                skew_shift = (data_skew[j] * skew_x) - data_skew[j]     #calc skew_shift based on skew_x value\n",
    "                skew_shift = data_avg[j] * skew_shift * gap2            #Apply skew_shift to average value, to get shift in unit_scale\n",
    "                \n",
    "                val = data[idx][j] + (diff*gap) + skew_shift\n",
    "                synth[i].append(val)\n",
    "                \n",
    "        #Make into Dataframe\n",
    "        '''Append synth array onto the original dataset array'''\n",
    "        len_data=len(self.data)\n",
    "        for i in range(self.synth_n):\n",
    "            self.data.loc[len_data+i]=synth[i]\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a445b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging petting and non_petting datasets into one dataframe\n",
    "def into_one_dataframe(k, pad, synth_size, skew, t_1, t_0):\n",
    "    p=SMOTE(t_1, k, pad, synth_size+5, skew)\n",
    "    q=SMOTE(t_0, k, pad, synth_size, skew)\n",
    "    petting_data=p.smote_al()\n",
    "    non_petting_data=q.smote_al()\n",
    "    petting_data['Petting'] = 1\n",
    "    non_petting_data['Petting'] = 0\n",
    "    merged_data=pd.concat([petting_data,non_petting_data]) \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674e768",
   "metadata": {},
   "source": [
    "## Create Custom Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "842462db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab86473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff_synthData(dataset):    \n",
    "    X_train = dataset.drop('Petting', axis=1)\n",
    "    y_train = dataset['Petting']    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "031c2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble(paramSet, cnt, t_1, t_0):\n",
    "    # Create ensembles\n",
    "    ensemble = []\n",
    "    num_trees = len(paramSet)\n",
    "    xtest = []\n",
    "    ytest = []\n",
    "    # Generate a different synthetic dataset for each tree in the ensemble\n",
    "    while cnt>0:\n",
    "        for i in range(num_trees):\n",
    "            \n",
    "            # Create synth dataset for each tree\n",
    "            data_after_smote = into_one_dataframe(paramSet[i][0], paramSet[i][1], \n",
    "                                                  paramSet[i][2], paramSet[i][3], t_1.copy(), t_0.copy())\n",
    "            X, y = create_diff_synthData(data_after_smote)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "            xtest.append(X_test)\n",
    "            ytest.append(y_test)\n",
    "            \n",
    "            #Decision Tree\n",
    "            tree = DecisionTreeClassifier()\n",
    "            tree.fit(X_train, y_train)\n",
    "            ensemble.append(tree)\n",
    "        cnt-=1\n",
    "    return ensemble, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e052944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_test_list, y_test_list, ensemble):\n",
    "    # Evaluating ensemple\n",
    "    acc = np.zeros((len(X_test_list), len(ensemble)))\n",
    "    auc = np.zeros((len(X_test_list), len(ensemble)))    \n",
    "    for i, tree in enumerate(ensemble):\n",
    "        # Confusion Matirx\n",
    "        y_pred = tree.predict(X_test_list[i])\n",
    "        y_pred_prob = tree.predict_proba(X_test_list[i])[:, 1]\n",
    "        acc[:, i] = accuracy_score(y_test_list[i], y_pred)\n",
    "        auc[:, i] = roc_auc_score(y_test_list[i], y_pred_prob)\n",
    "\n",
    "    #Evaluate\n",
    "    accuracy = np.mean(acc)\n",
    "    auc_mean = np.mean(auc)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"AUC: \", auc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb7c83",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1675cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_7(paramSet, cnt, t_1, t_0):\n",
    "    ensemble, X_test_list, y_test_list = create_ensemble(paramSet, cnt, t_1, t_0)\n",
    "    evaluate(X_test_list, y_test_list, ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "363c4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "petting=data_preprocessing(petting_d)\n",
    "non_petting=data_preprocessing(non_petting_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f9c9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one by one\n",
    "# [k, pad, synth_size, skew]\n",
    "params_list = [[2, 1, 40, 0.01], [2, 1, 40, 0.5], [2, 1, 40, 1.0], [2, 1, 40, 1.5], [2, 1, 40, 2.0],\n",
    "              [2, 1, 80, 0.01], [2, 1, 80, 0.5], [2, 1, 80, 1.0], [2, 1, 80, 1.5], [2, 1, 80, 2.0],\n",
    "              [2, 2, 40, 0.01], [2, 2, 40, 0.5], [2, 2, 40, 1.0], [2, 2, 40, 1.5], [2, 2, 40, 2.0],\n",
    "              [2, 2, 80, 0.01], [2, 2, 80, 0.5], [2, 2, 80, 1.0], [2, 2, 80, 1.5], [2, 2, 80, 2.0],\n",
    "              [3, 1, 40, 0.01], [3, 1, 40, 0.5], [3, 1, 40, 1.0], [3, 1, 40, 1.5], [3, 1, 40, 2.0],\n",
    "              [3, 1, 80, 0.01], [3, 1, 80, 0.5], [3, 1, 80, 1.0], [3, 1, 80, 1.5], [3, 1, 80, 2.0],\n",
    "              [3, 2, 40, 0.01], [3, 2, 40, 0.5], [3, 2, 40, 1.0], [3, 2, 40, 1.5], [3, 2, 40, 2.0],\n",
    "              [3, 2, 80, 0.01], [3, 2, 80, 0.5], [3, 2, 80, 1.0], [3, 2, 80, 1.5], [3, 2, 80, 2.0],\n",
    "              [4, 1, 40, 0.01], [4, 1, 40, 0.5], [4, 1, 40, 1.0], [4, 1, 40, 1.5], [4, 1, 40, 2.0],\n",
    "              [4, 1, 80, 0.01], [4, 1, 80, 0.5], [4, 1, 80, 1.0], [4, 1, 80, 1.5], [4, 1, 80, 2.0],\n",
    "              [4, 2, 40, 0.01], [4, 2, 40, 0.5], [4, 2, 40, 1.0], [4, 2, 40, 1.5], [4, 2, 40, 2.0],\n",
    "              [4, 2, 80, 0.01], [4, 2, 80, 0.5], [4, 2, 80, 1.0], [4, 2, 80, 1.5], [4, 2, 80, 2.0]]\n",
    "# Size duplication ## ex) 2 times or 3 times --> 120 trees / 180 trees\n",
    "cnt = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d46e54",
   "metadata": {},
   "source": [
    "## ACC, AUC Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "969bd18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8856172839506172\n",
      "AUC:  0.8989940604792239\n"
     ]
    }
   ],
   "source": [
    "main_7(params_list, cnt, petting, non_petting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
